Alignment: making AI systems try to do what their creators intend them to do (some people call this intent alignment).

 the problem of AI Capabilities:: developing AI systems that effectively carry out the tasks that they are trying to achieve.[4]
---
Examples of competence failures: medical AI systems that give dangerous recommendations, or house price estimators that lead to significant financial losses.

Moral philosophy: determining what intentions would be "good" for AI systems to have. 
---
 if we can make AI systems do what we intend (alignment), we then need to know what we should intend these systems to do.

Governance: deterring harmful development or use of AI systems.
---
. Examples of governance failures: reckless or malicious development of dangerous AI systems, or deploying systems without safeguards against serious misuse. 

Resilience: preparing other systems to cope with the negative impacts of AI.
---

inner and outer alignment 
Outer alignment: Specify goals to an AI system correctly.
Also known as solving: reward misspecification, reward hacking, specification gaming, Goodharting.

Inner alignment: Get AI to follow these goals.
Also known as solving: goal misgeneralization.